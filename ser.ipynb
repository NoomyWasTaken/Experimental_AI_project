{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATeBxOh7Y-RR",
        "outputId": "c41aa732-fe23-4ba5-9856-98f59a030408"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tangl\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.output.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.output.bias']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.weight', 'projector.bias', 'projector.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForAudioClassification, Wav2Vec2FeatureExtractor\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch.nn as nn\n",
        "\n",
        "# https://github.com/ehcalabres/EMOVoice\n",
        "# the preprocessor was derived from https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english\n",
        "# processor1 = AutoProcessor.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
        "# ^^^ no preload model available for this model (above), but the `feature_extractor` works in place\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForAudioClassification.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "\n",
        "model.projector = nn.Linear(1024, 1024, bias=True)\n",
        "model.classifier = nn.Linear(1024, 8, bias=True)\n",
        "\n",
        "torch_state_dict = torch.load('pytorch_model.bin')\n",
        "\n",
        "model.projector.weight.data = torch_state_dict['classifier.dense.weight']\n",
        "model.projector.bias.data = torch_state_dict['classifier.dense.bias']\n",
        "\n",
        "model.classifier.weight.data = torch_state_dict['classifier.output.weight']\n",
        "model.classifier.bias.data = torch_state_dict['classifier.output.bias']\n",
        "\n",
        "model.to(device)\n",
        "print(device)\n",
        "\n",
        "def predict_emotion(audio_file):\n",
        "    speech, sr = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "    input = feature_extractor(\n",
        "        raw_speech=speech,\n",
        "        sampling_rate=16000,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\")\n",
        "\n",
        "    result = model.forward(torch.tensor(input.input_values).to(device))\n",
        "    # making sense of the result\n",
        "    id2label = {\n",
        "        \"0\": \"angry\",\n",
        "        \"1\": \"calm\",\n",
        "        \"2\": \"disgust\",\n",
        "        \"3\": \"fearful\",\n",
        "        \"4\": \"happy\",\n",
        "        \"5\": \"neutral\",\n",
        "        \"6\": \"sad\",\n",
        "        \"7\": \"surprised\"\n",
        "    }\n",
        "    interp = dict(zip(id2label.values(), list(round(float(i),4) for i in result[0][0])))\n",
        "    return interp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WAo_5bmoeVZ4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'angry': 0.4614, 'calm': -1.7554, 'disgust': -2.4155, 'fearful': -0.8766, 'happy': 2.6283, 'neutral': 0.4152, 'sad': -3.0915, 'surprised': 3.9948}\n",
            "surprised\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\tangl\\AppData\\Local\\Temp\\ipykernel_16640\\2835020544.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  result = model.forward(torch.tensor(input.input_values).to(device))\n"
          ]
        }
      ],
      "source": [
        "interp = predict_emotion('../Audio_Speech_Actors_01-24/Actor_01/03-01-08-02-01-01-01.wav')\n",
        "print(interp)\n",
        "print(max(interp, key=interp.get))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6myOimAeAP5"
      },
      "source": [
        "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
